{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naver movie 개봉 전 review Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class NaverMovieScraping:    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        \n",
    "    def URLcode(self, url):\n",
    "        self.url = url\n",
    "        start = re.search('code=[0-9]+',self.url).group()\n",
    "        code = re.search('[0-9]+',start).group()\n",
    "        return code \n",
    "    \n",
    "    def ReviewSCORE(self, code):\n",
    "        \n",
    "        self.code = code\n",
    "        url_base = \"http://movie.naver.com/movie/bi/mi/point.nhn?code={}\".format(str(code))\n",
    "        html = urlopen(url_base).read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        graph = soup.findAll('span',{'class':'exp_cnt'})\n",
    "        star = soup.findAll('div', {'class':'sc_area b_star'})\n",
    "        movie_info = soup.findAll('dl', {'class' : 'info_spec'})\n",
    "        exp_idx = [elem.text for elem in graph]\n",
    "        star_idx = [s.text for s in star]\n",
    "       \n",
    "        try:\n",
    "            smile = ''.join(re.findall('[0-9]',exp_idx[0]))\n",
    "            not_smile= ''.join(re.findall('[0-9]',exp_idx[1])) \n",
    "            star_val = star_idx[0][17:21]\n",
    "            num = star_idx[0][26:31]\n",
    "\n",
    "        except IndexError as ex:\n",
    "            smile = 0\n",
    "            not_smile = 0\n",
    "            star_val = 0\n",
    "            num = 0\n",
    "            \n",
    "        try:\n",
    "            text = [info.get_text().replace('\\n', '').replace('\\t', '').replace('\\r','') for info in movie_info]\n",
    "                           \n",
    "        except ValueError as ex:\n",
    "            text = []\n",
    "            \n",
    "        return pd.DataFrame({'보고싶어요' : [smile], \n",
    "                             '글쎄요' : [not_smile], \n",
    "                             '평점' : [star_val], \n",
    "                             '참여인원' : [num],\n",
    "                             '영화정보' :[text]})\n",
    "    \n",
    "    def getActors(self, code):\n",
    "        \n",
    "        self.code = code\n",
    "        url_role = \"http://movie.naver.com/movie/bi/mi/detail.nhn?code={}\".format(str(code))\n",
    "        html_role = urlopen(url_role).read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html_role, 'lxml')\n",
    "        actors = soup.findAll('a', {'class':'k_name'})\n",
    "        roles = soup.findAll('p', {'class':'in_prt'})\n",
    "        \n",
    "        actor = [act.text for act in actors]\n",
    "        role = [r.text.replace('\\n','') for r in roles]\n",
    "        \n",
    "        return pd.DataFrame({'배우' : [actor[:len(actor)-1]],\n",
    "                             '역할' : [role],\n",
    "                             '감독' : [actor[len(actor)-1]]})\n",
    "    \n",
    "    def review_page(self, code):\n",
    "        \n",
    "        self.code = code\n",
    "        # url의 경우 따로 개봉전 주소를 넣는다.\n",
    "        url_base = \"http://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code={}&type=before\".format(str(code))\n",
    "        html = urlopen(url_base).read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        score_total = soup.findAll('div', {'class' : 'score_total'})\n",
    "        review_num = [elem.findChildren('em')[1].get_text() for elem in score_total]\n",
    "    \n",
    "        # review_num이 1000개를 넘어가면 , 문자를 제거 \n",
    "        return math.ceil(int(review_num[0].replace(\",\",\"\")) / 10)\n",
    "    \n",
    "    def getReview(self, code, page_num):\n",
    "    \n",
    "        page = int(1)\n",
    "        #count = int(input('page number : '))\n",
    "        self.page_num = int(page_num) # review의 전체 페이지\n",
    "    \n",
    "        reple_list = []\n",
    "        score_list = []\n",
    "        good_list = []\n",
    "        bad_list = []\n",
    "        self.code = code\n",
    "        \n",
    "        while page_num:\n",
    "        \n",
    "            url_base = \"http://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code={}&type=before&page={}\".format(str(code),str(page))\n",
    "            html = urlopen(url_base).read().decode('utf-8')\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "            score_result = soup.find('div', {'class' : 'score_result'})\n",
    "            li_list = score_result.find_all('li')\n",
    "        \n",
    "            for li in li_list:\n",
    "            \n",
    "                page = int(page)\n",
    "                reple = li.find('div', {'class' : 'score_reple'}).find('p').get_text()\n",
    "            \n",
    "            # 별점이 없는 곳은 0점으로 넣는다.\n",
    "                try:\n",
    "                    score = li.find('div', {'class' : 'star_score'}).find('em').get_text()\n",
    "                except Exception as ex:\n",
    "                    score = 0\n",
    "                \n",
    "                good = li.find('div', {'class' : 'btn_area'}).get_text().split('\\n')[1]\n",
    "                bad = li.find('div', {'class' : 'btn_area'}).get_text().split('\\n')[2]\n",
    "            \n",
    "                reple_list.append(reple)\n",
    "                score_list.append(score)\n",
    "                good_list.append(''.join([s for s in str(good) if s.isdigit()]))\n",
    "                bad_list.append(''.join([s for s in str(bad) if s.isdigit()]))\n",
    "            \n",
    "            page_num -= 1\n",
    "        \n",
    "            if not page_num:\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "    \n",
    "        return pd.DataFrame({'score' : score_list, 'reple' : reple_list, \n",
    "                             'good' : good_list, 'bad' : bad_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Naver movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(os.getcwd())\n",
    "movie_df = pd.read_csv('url_list.csv', encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_score(movie_df):\n",
    "    \n",
    "    Review_df= pd.DataFrame()\n",
    "    \n",
    "    for i, url in enumerate(movie_df['url']):\n",
    "        print(1)\n",
    "        print('url 을 긁었습니다.', 'pos :', i)\n",
    "        crawler = NaverMovieScraping(url)\n",
    "        Review_df = pd.concat([Review_df, crawler.ReviewSCORE(crawler.URLcode(url))],ignore_index=True)\n",
    "    \n",
    "    return(Review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_actor(movie_df):\n",
    "    \n",
    "    Review_df= pd.DataFrame()\n",
    "    \n",
    "    for i, url in enumerate(movie_df['url']):\n",
    "        print('url 을 긁었습니다.', 'pos :', i)\n",
    "        crawler = NaverMovieScraping(url)\n",
    "        Review_df = pd.concat([Review_df, crawler.getActors(crawler.URLcode(url))],ignore_index=True)\n",
    "    \n",
    "    return(Review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = movie_score(movie_df)\n",
    "actor = movie_actor(movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"title\"] = movie_df[\"title\"]\n",
    "data[\"url\"] = movie_df[\"url\"]\n",
    "data[\"y\"] = movie_df[\"Y (손익분기점)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([data, actor], axis=1) # 영화정보 DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reivew 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_review(movie_df):\n",
    "    \n",
    "    Review_df= pd.DataFrame()\n",
    "    id = pd.DataFrame(movie_df[\"title\"])\n",
    "    for i, url in enumerate(movie_df['url']):\n",
    "        print('url 을 긁었습니다.', 'pos :', i)\n",
    "        crawler = NaverMovieScraping(url)\n",
    "        URL = crawler.URLcode(url)\n",
    "        page = crawler.review_page(URL)\n",
    "        print(page)\n",
    "        review = crawler.getReview(URL,page)\n",
    "        review[\"id\"] = movie_df[\"title\"][i]\n",
    "        review['y'] = movie_df[\"y\"][i]\n",
    "        Review_df = pd.concat([Review_df, review],ignore_index=True)\n",
    "    return(Review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = movie_review(df) # 영화리뷰 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.to_csv(\"review.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
